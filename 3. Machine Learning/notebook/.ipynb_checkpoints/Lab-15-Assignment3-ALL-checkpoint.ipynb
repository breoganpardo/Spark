{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 16 Assignment 3 - Group Assignment\n",
    "\n",
    "When creating ML models, the concept of efficiency has three sides:\n",
    "1. The time dedicated by the analyst to build the model\n",
    "2. The computer time and resources needed by the final model\n",
    "3. The accuracy of the final model\n",
    "\n",
    "Efficiency is a combination of all\n",
    "\n",
    "In this assignment, you are asked to be efficient. Spark is the best tool to build models over massive datasets\n",
    "\n",
    "If you need to create Spark+Python Machine Learning models that \"run fast\" on the  cluster, you must avoid using Python code or working with RRD+python. Try to use  the already existing methods that do what you need (do not reinvent the wheel).\n",
    "\n",
    "Therefore try to use the implemented object+methods inside the Spark SQL and ML modules. They are very fast, because it is compiled Java/Scala code. Try to use: DataFrames, Feature Transfomers, Estimators, Pipelines, GridSearch, CV, ...\n",
    "\n",
    "For this assignment, you are asked to create a classification model that:\n",
    "1. Uses the variables in the dataset (train.csv) to predict label \"loan_status\"\n",
    "2. Write a python scripts that:\n",
    "    - Reads the \"train.csv\" and \"test.csv\" files, transform and select variables as you wish.\n",
    "    - Train/fit your model using the \"train.csv\".\n",
    "    - Predict your model on the \"test.csv\" ( you should generate a file with your predictions).\n",
    "    - I will use a different test dataset (with the true loan_status).\n",
    "\n",
    "Your work will be evaluated under the following scoring schema\n",
    "- (40%) ETL process\n",
    "- (40%) Model train process\n",
    "- (10%) Code Readability \n",
    "- (10%) AUC on the test set (at least 50%)\n",
    "\n",
    "Enjoy it and best of luck!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Assignment is based on kaggle competition https://www.kaggle.com/c/loan-default-prediction from where a sub-dataset has been taken.\n",
    "\n",
    "### File descriptions\n",
    "**train.csv** - the training set (to use for building a model)\n",
    "\n",
    "**test.csv** - the test set (to use for applying predictings)\n",
    "\n",
    "**sample_submission.csv** - a template for the submission file\n",
    "\n",
    "### Data Description (also contained in LendingClub_DataDescription.csv)\n",
    "**ID**: A unique LC assigned ID for the loan listing.\n",
    "\n",
    "**loan_amnt**: The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.\n",
    "\n",
    "**loan_status**: Current status of the loan (**Target**: 1 = Charged Off, 0 = Fully Paid).\n",
    "\n",
    "**term**: The number of payments on the loan. Values are in months and can be either 36 or 60.\n",
    "\n",
    "**int_rate**: Interest Rate on the loan.\n",
    "\n",
    "**installment**: The monthly payment owed by the borrower if the loan originates.\n",
    "\n",
    "**emp_length**: Employment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years.\n",
    "\n",
    "**home_ownership**: The home ownership status provided by the borrower during registration. Our values are: OTHER/NONE, MORTGAGE, OWN, RENT.\n",
    "\n",
    "**annual_inc**: The self-reported annual income provided by the borrower during registration.\n",
    "\n",
    "**purpose**: A category provided by the borrower for the loan request.\n",
    "\n",
    "**title**: The loan title provided by the borrower.\n",
    "\n",
    "**STATE**: The state provided by the borrower in the loan application.\n",
    "\n",
    "**delinq_2yrs**: The number of 30+ days past-due incidences of delinquency in the borrower's credit file for the past 2 years.\n",
    "\n",
    "**revol_bal**: Total credit revolving balance.\n",
    "\n",
    "**revol_util**: Revolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit.\n",
    "\n",
    "**total_pymnt**: Indicates total payment at the end of the loan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c817e3a17189>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m#Initialize SparkSession and SparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m#Create a Spark Session\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['SPARK_HOME'] = \"/Users/ailleren/Documents/Spark/spark-2.3.2-bin-hadoop2.7\"\n",
    "\n",
    "# Create a variable for our root path\n",
    "SPARK_HOME = os.environ['SPARK_HOME']\n",
    "\n",
    "#Add the following paths to the system path. Please check your installation\n",
    "#to make sure that these zip files actually exist. The names might change\n",
    "#as versions change.\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\",\"lib\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\",\"lib\",\"pyspark.zip\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\",\"lib\",\"py4j-0.10.7-src.zip\"))\n",
    "\n",
    "#Initialize SparkSession and SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Create a Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"MiPrimer\") \\\n",
    "    .config(\"spark.executor.memory\", \"6g\") \\\n",
    "    .config(\"spark.cores.max\",\"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "#Get the Spark Context from Spark Session    \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.ml.feature import Imputer\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read the files into a Dataframes and show them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loanDF = spark.read.format('csv') \\\n",
    "           .option(\"inferSchema\", \"true\") \\\n",
    "           .option(\"header\",\"true\")\\\n",
    "           .option(\"delimiter\", \";\") \\\n",
    "           .load('../data/train.csv') \n",
    "\n",
    "testDF = spark.read.format('csv') \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .option(\"header\",\"true\")\\\n",
    "            .option(\"delimiter\", \";\") \\\n",
    "            .load('../data/test.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loanDF.limit(200).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loanDF.dtypes\n",
    "\n",
    "loanDF=loanDF.withColumn('int_rate', col('int_rate').substr(1, 4))\\\n",
    "             .withColumn(\"int_rate\", col(\"int_rate\").cast('float'))\\\n",
    "             .withColumn('revol_util', col('revol_util').substr(1, 4))\\\n",
    "             .withColumn(\"revol_util\", col(\"revol_util\").cast('float'))\\\n",
    "             .withColumn(\"delinq_2yrs\", col(\"delinq_2yrs\").cast('float'))\\\n",
    "             .withColumn(\"revol_bal\", col(\"revol_bal\").cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loanDF.toPandas().describe().round(decimals=0).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Analizing null values, cross tables distribution and covariances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing missing and null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loanDF_empty=loanDF.select([f.count(f.when(col(c).isNull() | f.isnan(c), c)).alias(c) for c in loanDF.columns])\n",
    "print('Total number of rows = {0}. Total number of missing values, per column:'.format(loanDF.count()))\n",
    "loanDF_empty.toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missings treatment\n",
    "\n",
    "empty_values=[\"delinq_2yrs\",\"revol_bal\",\"revol_util\",\"total_pymnt\"]\n",
    "null_imputer=Imputer()\\\n",
    "        .setStrategy('median')\\\n",
    "        .setInputCols(empty_values)\\\n",
    "        .setOutputCols(empty_values)\\\n",
    "        .fit(loanDF)\n",
    "\n",
    "loanDF=null_imputer.transform(loanDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=loanDF['loan_amnt','int_rate','installment','annual_inc','delinq_2yrs','revol_bal','revol_util','total_pymnt'].toPandas()\n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.2,left=0.1, right=2,top=2.9, bottom=0.1)\n",
    "for i in range(0,8):\n",
    "    plt.subplot(4,2,i+1)    \n",
    "    plt.boxplot(df.iloc[:,i])\n",
    "    plt.title(label=(df.columns[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outlier treatment: Winsorization approach\n",
    "\n",
    "#loan_amnt    \n",
    "df=loanDF\n",
    "val='loan_amnt'\n",
    "quantiles = df.approxQuantile(val, [0.05,0.95],0.0)\n",
    "lower_thres = quantiles[0]\n",
    "upper_thres = quantiles[1]\n",
    "loanDF = df.withColumn(val,when(df[val] >= upper_thres,upper_thres).otherwise(df[val]))\n",
    "\n",
    "#int_rate\n",
    "df=loanDF\n",
    "val='int_rate'\n",
    "quantiles = df.approxQuantile(val, [0.05,0.95],0.0)\n",
    "lower_thres = quantiles[0]\n",
    "upper_thres = quantiles[1]\n",
    "loanDF = df.withColumn(val,when(df[val] >= upper_thres,upper_thres).otherwise(df[val]))\n",
    "\n",
    "#installment\n",
    "df=loanDF\n",
    "val='installment'\n",
    "quantiles = df.approxQuantile(val, [0.05,0.95],0.0)\n",
    "lower_thres = quantiles[0]\n",
    "upper_thres = quantiles[1]\n",
    "loanDF = df.withColumn(val,when(df[val] >= upper_thres,upper_thres).otherwise(df[val]))\n",
    "\n",
    "#installment\n",
    "df=loanDF\n",
    "val='installment'\n",
    "quantiles = df.approxQuantile(val, [0.05,0.95],0.0)\n",
    "lower_thres = quantiles[0]\n",
    "upper_thres = quantiles[1]\n",
    "loanDF = df.withColumn(val,when(df[val] >= upper_thres,upper_thres).otherwise(df[val]))\n",
    "\n",
    "#revol_bal\n",
    "df=loanDF\n",
    "val='revol_bal'\n",
    "quantiles = df.approxQuantile(val, [0.05,0.95],0.0)\n",
    "lower_thres = quantiles[0]\n",
    "upper_thres = quantiles[1]\n",
    "loanDF = df.withColumn(val,when(df[val] >= upper_thres,upper_thres).otherwise(df[val]))\n",
    "\n",
    "#total_pymnt\n",
    "df=loanDF\n",
    "val='total_pymnt'\n",
    "quantiles = df.approxQuantile(val, [0.05,0.95],0.0)\n",
    "lower_thres = quantiles[0]\n",
    "upper_thres = quantiles[1]\n",
    "loanDF = df.withColumn(val,when(df[val] >= upper_thres,upper_thres).otherwise(df[val]))\n",
    "\n",
    "#annual_inc\n",
    "df=loanDF\n",
    "val=\"annual_inc\"\n",
    "quantiles = df.approxQuantile(val, [0.05,0.95],0.0)\n",
    "lower_thres = quantiles[0]\n",
    "upper_thres = quantiles[1]\n",
    "\n",
    "loanDF = df.withColumn(val,when(df[val] >= upper_thres,upper_thres).otherwise(df[val]))\n",
    "loanDF = df.withColumn(val,when(df[val] <= lower_thres,lower_thres).otherwise(df[val]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validating Outliers Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=loanDF['loan_amnt','int_rate','installment','annual_inc','delinq_2yrs','revol_bal','revol_util','total_pymnt'].toPandas()\n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.2,left=0.1, right=2,top=2.9, bottom=0.1)\n",
    "for i in range(0,8):\n",
    "    plt.subplot(4,2,i+1)    \n",
    "    plt.boxplot(df.iloc[:,i])\n",
    "    plt.title(label=(df.columns[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ETL summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should explain how you are going to clean and prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Spark code about the one explained in \"ETL Summary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Spark code about the transformations you apply to the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new variable: ratio_installment/monthly_income\n",
    "\n",
    "#r_ins_minc\n",
    "loanDF=loanDF.withColumn('r_ins_minc',(loanDF['installment']/(loanDF['annual_inc']/12)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numerical variables into categorical\n",
    "\n",
    "#int_rate\n",
    "val='int_rate'\n",
    "splits=[-float(\"inf\"),10.0,15.0,float(\"inf\")]\n",
    "bucketizer1=Bucketizer(splits=splits,inputCol=val,outputCol=val+'_disc')\n",
    "loanDF=bucketizer1.transform(loanDF)\n",
    "\n",
    "#delinq_2yrs\n",
    "val='delinq_2yrs'\n",
    "splits=[0.0,1.0,2.0,float(\"inf\")]\n",
    "bucketizer2=Bucketizer(splits=splits,inputCol=val,outputCol=val+'_disc')\n",
    "loanDF=bucketizer2.transform(loanDF)\n",
    "\n",
    "#r_ins_minc\n",
    "val='r_ins_minc'\n",
    "splits=[-float(\"inf\"),0.05,0.1,float(\"inf\")]\n",
    "bucketizer3=Bucketizer(splits=splits,inputCol=val,outputCol=val+'_disc')\n",
    "loanDF=bucketizer3.transform(loanDF)\n",
    "\n",
    "#revol_util\n",
    "val='revol_util'\n",
    "splits=[-float(\"inf\"),0.2,0.6,float(\"inf\")]\n",
    "bucketizer4=Bucketizer(splits=splits,inputCol=val,outputCol=val+'_disc')\n",
    "loanDF=bucketizer4.transform(loanDF)\n",
    "\n",
    "#revol_bal\n",
    "val='revol_bal'\n",
    "splits=[-float(\"inf\"),10000,float(\"inf\")]\n",
    "bucketizer4=Bucketizer(splits=splits,inputCol=val,outputCol=val+'_disc')\n",
    "loanDF=bucketizer4.transform(loanDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regrouping categorical variables\n",
    "\n",
    "#term\n",
    "df=loanDF\n",
    "column='term'\n",
    "loanDF = df.withColumn('term_g', (when(df[column] == ' 36 months', 0).otherwise(1)))\n",
    "\n",
    "#purpose\n",
    "df=loanDF\n",
    "column='purpose'\n",
    "loanDF = df.withColumn('purpose_g',\\\n",
    "            (when((df[column] == 'small_business') | (df[column] == 'renewable_energy'), 0)\\\n",
    "            .otherwise(when((df[column] == 'educational') | (df[column] == 'house') |(df[column] == 'moving')\\\n",
    "                            |(df[column] == 'other') |(df[column] == 'vacation') | (df[column] == 'debt_consolidation')\\\n",
    "                            | (df[column] == 'medical') | (df[column] == ''), 1).otherwise(2))))\n",
    "\n",
    "#STATE\n",
    "df=loanDF\n",
    "column='STATE'\n",
    "loanDF = df.withColumn('states_g',\\\n",
    "            (when((df[column] == 'NE') | (df[column] == 'NV') | (df[column] == 'AK'), 0)\\\n",
    "            .otherwise(when((df[column] == 'FL') | (df[column] == 'OR') |(df[column] == 'MO')\\\n",
    "                            |(df[column] == 'WA') |(df[column] == 'SD') | (df[column] == 'CA')\\\n",
    "                            | (df[column] == 'GA') | (df[column] == 'UT') | (df[column] == 'NJ'), 1)\\\n",
    "            .otherwise(when((df[column] == 'MD') | (df[column] == 'NC') |(df[column] == 'SC')\\\n",
    "                            |(df[column] == 'MN') |(df[column] == 'NI') | (df[column] == 'TN')\\\n",
    "                            | (df[column] == 'OH') | (df[column] == '') | (df[column] == 'KY')\\\n",
    "                            | (df[column] == 'WY') | (df[column] == 'NY') | (df[column] == 'OK')\\\n",
    "                            | (df[column] == 'AZ') | (df[column] == 'NM') | (df[column] == 'KS')\\\n",
    "                            | (df[column] == 'HI') | (df[column] == 'CT'), 2)\\\n",
    "            .otherwise(when((df[column] == 'IL') | (df[column] == 'VA') |(df[column] == 'VT')\\\n",
    "                            |(df[column] == 'AL') |(df[column] == 'MA') | (df[column] == 'RI')\\\n",
    "                            | (df[column] == 'CO') | (df[column] == 'TX') | (df[column] == 'MT')\\\n",
    "                            | (df[column] == 'WV') | (df[column] == 'PA') | (df[column] == 'NH')\\\n",
    "                            | (df[column] == 'AZ') | (df[column] == 'LA') | (df[column] == 'AR'),3)\\\n",
    "                            \n",
    "            .otherwise(4))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loanDF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ae154b702000>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Drop variables no longer needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m loanDF = loanDF.drop('ID','term','loan_amnt','title','int_rate','installment','emp_length',\\\n\u001b[0m\u001b[0;32m      3\u001b[0m                       \u001b[1;34m'home_ownership'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'annual_inc'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'purpose'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'STATE'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'delinq_2yrs'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                       'revol_bal','revol_util','total_pymnt','r_ins_minc')\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loanDF' is not defined"
     ]
    }
   ],
   "source": [
    "#Drop variables no longer needed\n",
    "loanDF = loanDF.drop('ID','term','loan_amnt','title','int_rate','installment','emp_length',\\\n",
    "                      'home_ownership','annual_inc','purpose','title','STATE','delinq_2yrs',\\\n",
    "                      'revol_bal','revol_util','total_pymnt','r_ins_minc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multivariate Analysis: Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = loanDF\n",
    "col_names = df.columns\n",
    "features = df.rdd.map(lambda row: row[0:])\n",
    "corr_mat=Statistics.corr(features, method=\"pearson\")\n",
    "corr_df = pd.DataFrame(corr_mat)\n",
    "corr_df.index, corr_df.columns = col_names, col_names\n",
    "\n",
    "corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loanDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loanDF.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Code to assemble the variables to a numerical vector (VectorAssembler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression,LogisticRegressionSummary,RandomForestClassificationModel, RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n",
    "from pyspark.ml.param import Params\n",
    "from pyspark.ml import Pipeline, PipelineModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Create Pipeline for train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []\n",
    "assembler = VectorAssembler(inputCols=[\"int_rate_disc\",\"delinq_2yrs_disc\",\"r_ins_minc_disc\", \"revol_util_disc\",\n",
    "                                \"revol_bal_disc\", \"term_g\", \"purpose_g\",\"states_g\"], outputCol=\"features\")\n",
    "stages.append(assembler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier() \\\n",
    "        .setFeaturesCol(\"features\") \\\n",
    "        .setLabelCol(\"loan_status\") \\\n",
    "        .setSeed(100)\n",
    "stages.append(rf)\n",
    "\n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using metric: areaUnderROC\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator() \\\n",
    "                .setLabelCol(\"loan_status\") \\\n",
    "                .setRawPredictionCol(\"rawPrediction\")\n",
    "\n",
    "print(\"We are using metric: \" + evaluator.getMetricName())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20882\n",
      "8873\n"
     ]
    }
   ],
   "source": [
    "(trainingData, validationData) = loanDF.randomSplit([0.7, 0.3], seed=100)\n",
    "print(trainingData.count())\n",
    "print(validationData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(trainingData)\n",
    "#model.explainParams()\n",
    "pipeline_results = model.transform(validationData)\n",
    "\n",
    "#pipeline_results.toPandas()\n",
    "\n",
    "#pipeline_results.select('prediction').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.6773291555841279\n",
      "\n",
      "Model Parameters\n",
      "----------------\n",
      "{Param(parent='RandomForestClassifier_4e55a3fe26a12af2574f', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'probability', Param(parent='RandomForestClassifier_4e55a3fe26a12af2574f', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction', Param(parent='RandomForestClassifier_4e55a3fe26a12af2574f', name='seed', doc='random seed.'): 100, Param(parent='RandomForestClassifier_4e55a3fe26a12af2574f', name='predictionCol', doc='prediction column name.'): 'prediction', Param(parent='RandomForestClassifier_4e55a3fe26a12af2574f', name='labelCol', doc='label column name.'): 'loan_status', Param(parent='RandomForestClassifier_4e55a3fe26a12af2574f', name='featuresCol', doc='features column name.'): 'features', Param(parent='RandomForestClassifier_4e55a3fe26a12af2574f', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5, Param(parent='RandomForestClassifier_4e55a3fe26a12af2574f', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 32, Param(parent='RandomForestClassifier_4e55a3fe26a12af2574f', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 1, Param(parent='RandomForestClassifier_4e55a3fe26a12af2574f', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0, Param(parent='RandomForestClassifier_4e55a3fe26a12af2574f', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size.'): 256, Param(parent='RandomForestClassifier_4e55a3fe26a12af2574f', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval.'): False, Param(parent='RandomForestClassifier_4e55a3fe26a12af2574f', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext.'): 10, Param(parent='RandomForestClassifier_4e55a3fe26a12af2574f', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'gini', Param(parent='RandomForestClassifier_4e55a3fe26a12af2574f', name='numTrees', doc='Number of trees to train (>= 1).'): 20, Param(parent='RandomForestClassifier_4e55a3fe26a12af2574f', name='featureSubsetStrategy', doc='The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n].'): 'auto', Param(parent='RandomForestClassifier_4e55a3fe26a12af2574f', name='subsamplingRate', doc='Fraction of the training data used for learning each decision tree, in range (0, 1].'): 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"AUC: \" + str(evaluator.evaluate(pipeline_results)))\n",
    "\n",
    "print();print('Model Parameters')\n",
    "print('----------------')\n",
    "print(rf.extractParamMap())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Logistic Regresion Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Write a function \"metrics\" which has a LogisticRegressionModel.summary as input attribute and produces an output of: \n",
    "1. Area under ROC\n",
    "2. False Positive Rate By Label\n",
    "3. True Positive Rate By Label\n",
    "4. Precision By Label\n",
    "5. Recall By Label\n",
    "6. fMeasure By Label\n",
    "7. Accuracy\n",
    "8. False Positive Rate\n",
    "9. True Positive Rate\n",
    "10. fMeasure\n",
    "11. Precision\n",
    "12. Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(trainingSummary):\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Apply a Logistic Regresion Base Model and show the metrics by the function above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 We are going to try to improve our model:\n",
    "1. Using a `weight column` in our Logistic Regression Model (Take into account we are working with a unbalanced dataset)\n",
    "2. Define a `ParamGridBuilder` with `regParam`, `elasticNetParam` and `maxIter` at least\n",
    "3. Define an `BinaryClassificationEvaluator`\n",
    "4. Using Cross Validation with a 5-fold `CrossValidator`\n",
    "\n",
    "Questions to answer:\n",
    "1. Have we improved the ROC-AUC?\n",
    "2. Which are the average ROC-AUC measurements in the different cross validation runs?\n",
    "3. Which are the parameters of the best model in the 5 k-fold runs?\n",
    "4. Which are the metrics of the best model (training) in the 5 k-fold runs? (Use the function above)\n",
    "5. Which is the ROC-AUC on validation dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Random Forest Model\n",
    "1. Define a `ParamGridBuilder` with `maxDepth`, `numTrees` and `maxIter` at least\n",
    "2. Define an `BinaryClassificationEvaluator` (You can use the above one)\n",
    "3. Using Cross Validation with a 5-fold `CrossValidator`\n",
    "\n",
    "Questions to answer:\n",
    "\n",
    "1. Have we improved the ROC-AUC?\n",
    "2. Which are the average ROC-AUC measurements in the different cross validation runs?\n",
    "3. Which are the parameters of the best model in the 5 k-fold runs?\n",
    "4. Which is the importance of the features?\n",
    "5. Print full description of model.\n",
    "6. Which is the ROC-AUC on validation dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Gradient Boosting Model\n",
    "1. Defining a `ParamGridBuilder` with `maxDepth`, `numTrees` and `maxIter` at least (You can use the above one)\n",
    "2. Define an `BinaryClassificationEvaluator` (You can use the above one)\n",
    "3. Using Cross Validation with a 5-fold `CrossValidator`\n",
    "\n",
    "Questions to answer:\n",
    "\n",
    "1. Have we improved the ROC-AUC?\n",
    "2. Which are the average ROC-AUC measurements in the different cross validation runs?\n",
    "3. Which are the parameters of the best model in the 5 k-fold runs?\n",
    "4. Which is the importance of the features?\n",
    "5. Print full description of model.\n",
    "6. Which is the ROC-AUC on validation dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Apply your best model to send the predictions on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
